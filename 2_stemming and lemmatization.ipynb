{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "7a719004",
   "metadata": {},
   "source": [
    "# Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "004e1586",
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6419f579",
   "metadata": {},
   "source": [
    "# Load And tokenized the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b3d3dedc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Over the last quarter Apple sold nearly 20 thousand iPods for a profit of $6 million.']\n"
     ]
    }
   ],
   "source": [
    "TEXT = \"Over the last quarter Apple sold nearly 20 thousand iPods for a profit of $6 million.\"\n",
    "print(sent_tokenize(TEXT))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "09d033e4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Over\n",
      "the\n",
      "last\n",
      "quarter\n",
      "Apple\n",
      "sold\n",
      "nearly\n",
      "20\n",
      "thousand\n",
      "iPods\n",
      "for\n",
      "a\n",
      "profit\n",
      "of\n",
      "$\n",
      "6\n",
      "million\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "doc=nlp(TEXT)\n",
    "for i in doc:\n",
    "    print(i)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f7f76ca",
   "metadata": {},
   "source": [
    "# stemming Vs lemmatization\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1d7edc3",
   "metadata": {},
   "source": [
    "Stemming is preferred when you need a quick and straightforward way to reduce words to a common base form. It's useful in information retrieval tasks and text indexing where speed is crucial, but it may not always provide valid dictionary words.\n",
    "for example in sentence classification type projects.\n",
    "\n",
    "Lemmatization is the better choice when you require the correct base form of words for in-depth text analysis, semantic understanding, or tasks where word meaning is essential. It is more accurate but also computationally more intensive.\n",
    "For example chatbot,text generation type projects where dictionary is also matter."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae044d14",
   "metadata": {},
   "source": [
    "# stemming"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2c043cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "\n",
    "ps = PorterStemmer()\n",
    "s_stemmer = SnowballStemmer(language='english')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eda88341",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run --> run\n",
      "runner --> runner\n",
      "running --> run\n",
      "ran --> ran\n",
      "runs --> run\n",
      "easily --> easili\n",
      "fairly --> fairli\n"
     ]
    }
   ],
   "source": [
    "## applying porterstemmer\n",
    "words = ['run','runner','running','ran','runs','easily','fairly']\n",
    "for word in words:\n",
    "    print(word+' --> '+ps.stem(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "547e5b41",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run --> run\n",
      "runner --> runner\n",
      "running --> run\n",
      "ran --> ran\n",
      "runs --> run\n",
      "easily --> easili\n",
      "fairly --> fair\n"
     ]
    }
   ],
   "source": [
    "### applying SnowballStemmer\n",
    "for word in words:\n",
    "    print(word+' --> '+s_stemmer.stem(word))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6504325",
   "metadata": {},
   "source": [
    "# Lemitizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0589800c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run --> run\n",
      "runner --> runner\n",
      "running --> running\n",
      "ran --> ran\n",
      "runs --> run\n",
      "easily --> easily\n",
      "fairly --> fairly\n"
     ]
    }
   ],
   "source": [
    "from nltk import WordNetLemmatizer\n",
    "lemitizer=WordNetLemmatizer()\n",
    "\n",
    "for word in words:\n",
    "    print(word+' --> '+lemitizer.lemmatize(word))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1d6365fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run --> run\n",
      "runner --> runner\n",
      "running --> run\n",
      "ran --> run\n",
      "runs --> run\n",
      "easily --> easily\n",
      "fairly --> fairly\n"
     ]
    }
   ],
   "source": [
    "for word in words:\n",
    "    print(word+' --> '+lemitizer.lemmatize(word,pos='v'))    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1bfaad2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "## applying the sttemmer and limitier of our text tokens.\n",
    "\n",
    "def preprocess(stemmer):\n",
    "        processed_tokens = []\n",
    "\n",
    "        for token in doc:\n",
    "            if token.like_num:\n",
    "                # If the token is numeric, leave it as is\n",
    "                processed_tokens.append(token.text)\n",
    "            else:\n",
    "                # If the token is a word, apply stemming\n",
    "                processed_tokens.append(stemmer.stem(token.text))\n",
    "\n",
    "        processed_text = \" \".join(processed_tokens)\n",
    "        return processed_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "30f7ccd9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "over the last quarter appl sold nearli 20 thousand ipod for a profit of $ 6 million .\n",
      "over the last quarter appl sold near 20 thousand ipod for a profit of $ 6 million .\n"
     ]
    }
   ],
   "source": [
    "porter_text=preprocess(ps)\n",
    "print(porter_text)\n",
    "\n",
    "snowball_text=preprocess(s_stemmer)\n",
    "print(snowball_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "97b21f88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Over the last quarter Apple sold nearly 20 thousand iPods for a profit of $ 6 million .'"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "processed_tokens = []\n",
    "\n",
    "for token in doc:\n",
    "    if token.like_num:\n",
    "        # If the token is numeric, leave it as is\n",
    "        processed_tokens.append(token.text)\n",
    "    else:\n",
    "        # If the token is a word, apply stemming\n",
    "        processed_tokens.append(lemitizer.lemmatize(token.text))\n",
    "\n",
    "processed_text = \" \".join(processed_tokens)\n",
    "processed_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb1e56d4",
   "metadata": {},
   "source": [
    "# If we want to do the limitization with spacy library we can also do that , below is the code  for do this"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5fd29390",
   "metadata": {},
   "source": [
    "we already made the token with spacy libaray and we apply the limitization and stemming on that token using NLTK library , but we can get also the same thing with using sublibray that exist like token.lemma_ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b28aac70",
   "metadata": {},
   "outputs": [],
   "source": [
    "def show_lemmas(text):\n",
    "    for token in text:\n",
    "        print(f'{token.text:{12}} {token.pos_:{6}}  {token.lemma_}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "9e178303",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Over         ADP     over\n",
      "the          DET     the\n",
      "last         ADJ     last\n",
      "quarter      NOUN    quarter\n",
      "Apple        PROPN   Apple\n",
      "sold         VERB    sell\n",
      "nearly       ADV     nearly\n",
      "20           NUM     20\n",
      "thousand     NUM     thousand\n",
      "iPods        PROPN   iPods\n",
      "for          ADP     for\n",
      "a            DET     a\n",
      "profit       NOUN    profit\n",
      "of           ADP     of\n",
      "$            SYM     $\n",
      "6            NUM     6\n",
      "million      NUM     million\n",
      ".            PUNCT   .\n"
     ]
    }
   ],
   "source": [
    "show_lemmas(doc)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
