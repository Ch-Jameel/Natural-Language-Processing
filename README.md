**NLP Preprocessing Toolkit**

**Overview**

This repository contains a set of tools and scripts for basic Natural Language Processing (NLP) preprocessing steps commonly used to prepare text data for further analysis or model training. The primary steps covered include tokenization, stemming, lemmatization, vocabulary creation, and string matching using spaCy.

**Contents
Tokenization:**

Tokenization is the process of breaking down text into individual units, often words or subwords. This step is essential for subsequent NLP tasks.

**Stemming:**

Stemming reduces words to their base or root form. It involves removing suffixes to simplify word variations, making it useful for tasks like information retrieval.

**Lemmatization:**

Lemmatization is similar to stemming but involves reducing words to their base or dictionary form (lemma). It provides more accurate results compared to stemming but can be computationally more intensive.

**Vocabulary Creation and String Matching using spaCy:**


Utilize spaCy for creating a vocabulary and performing advanced string matching. spaCy provides efficient and accurate NLP functionalities.
