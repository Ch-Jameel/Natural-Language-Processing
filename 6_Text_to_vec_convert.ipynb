{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text To Vectors Conversion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Text-to-vector conversion is a fundamental step in Natural Language Processing (NLP) because machines cannot interpret raw text data directly. These conversions, such as One-Hot Encoding (OHE), Bag of Words (BoW), and Term Frequency-Inverse Document Frequency (TF-IDF) etc, allow us to transform text into numerical representations that machine learning algorithms can process. Each technique captures different aspects of the text."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are several methods that can be applied for text-to-vector conversion, each with its own advantages and disadvantages. The choice of method depends on the specific requirements and context of the task. We select the appropriate technique based on the nature of the data and the desired outcomes for our particular use case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[1.\tOHE](#One-Hot-Encoding)\n",
    "\n",
    "[2.\tBag-Of-Word](#Bag-of-Words)\n",
    "\n",
    "[3.\tTF-IDF](#TF-IDF)\n",
    "\n",
    "[4.\tWord2vec](#Word2vec)\n",
    "\n",
    "[5.\tAvgword2vec](#Avgword2vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "dummy_text_data = [\n",
    "    \"The quick brown fox jumps over the lazy dog.\",\n",
    "    \"The dog is lazy but the fox is quick.\",\n",
    "    \"Foxes are wild animals and dogs are domestic.\",\n",
    "    \"Both dogs and foxes have a tail.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to\n",
      "[nltk_data]     C:\\Users\\GH\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\GH\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n",
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\GH\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cleaned Data:\n",
      " ['quick brown fox jump lazy dog', 'dog lazy fox quick', 'fox wild animal dog domestic', 'dog fox tail']\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import string\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "\n",
    "# Download necessary NLTK data files\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('wordnet')\n",
    "\n",
    "def clean_text(text):\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
    "    \n",
    "    # Convert to lowercase\n",
    "    text = text.lower()\n",
    "    \n",
    "    # Tokenize the text\n",
    "    tokens = word_tokenize(text)\n",
    "    \n",
    "    # Remove stopwords\n",
    "    stop_words = set(stopwords.words('english'))\n",
    "    tokens = [word for word in tokens if word not in stop_words]\n",
    "    \n",
    "    # Lemmatization\n",
    "    lemmatizer = WordNetLemmatizer()\n",
    "    lemmatized_tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
    "    \n",
    "    return ' '.join(lemmatized_tokens)\n",
    "\n",
    "# Apply the cleaning function to the dummy data\n",
    "cleaned_data = [clean_text(text) for text in dummy_text_data]\n",
    "\n",
    "print(\"Cleaned Data:\\n\", cleaned_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "->\tStopword Removal: Words like \"the\", \"is\", \"but\", \"are\", and \"have\" have been removed.\n",
    "\n",
    "->\tLemmatization: Words like \"jumps\" → \"jump\" and \"foxes\" → \"fox\" have been lemmatized to their base forms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# One-Hot-Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One-Hot Encoding:\n",
      "   (0, 7)\t1.0\n",
      "  (1, 1)\t1.0\n",
      "  (2, 4)\t1.0\n",
      "  (3, 5)\t1.0\n",
      "  (4, 6)\t1.0\n",
      "  (5, 2)\t1.0\n",
      "  (6, 2)\t1.0\n",
      "  (7, 6)\t1.0\n",
      "  (8, 4)\t1.0\n",
      "  (9, 7)\t1.0\n",
      "  (10, 4)\t1.0\n",
      "  (11, 9)\t1.0\n",
      "  (12, 0)\t1.0\n",
      "  (13, 2)\t1.0\n",
      "  (14, 3)\t1.0\n",
      "  (15, 2)\t1.0\n",
      "  (16, 4)\t1.0\n",
      "  (17, 8)\t1.0\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "# Flatten the cleaned data into a list of words\n",
    "all_words = ' '.join(cleaned_data).split()\n",
    "\n",
    "# Reshape for OneHotEncoder\n",
    "ohe_input = [[word] for word in all_words]\n",
    "\n",
    "# Initialize and fit the encoder\n",
    "ohe_encoder = OneHotEncoder()\n",
    "ohe_data = ohe_encoder.fit_transform(ohe_input)\n",
    "\n",
    "# Display the One-Hot Encoded vectors\n",
    "print(\"One-Hot Encoding:\\n\", ohe_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pros:**\n",
    "\n",
    "- Simple and intuitive.\n",
    "\n",
    "- Represents the presence of words.\n",
    "\n",
    "**Cons:**\n",
    "\n",
    "- Creates very large and sparse matrices for large vocabularies.\n",
    "\n",
    "- Does not capture semantic relationships between words."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bag-of-Words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of Words:\n",
      " [[0 1 1 0 1 1 1 1 0 0]\n",
      " [0 0 1 0 1 0 1 1 0 0]\n",
      " [1 0 1 1 1 0 0 0 0 1]\n",
      " [0 0 1 0 1 0 0 0 1 0]]\n",
      "Feature Names: ['animal' 'brown' 'dog' 'domestic' 'fox' 'jump' 'lazy' 'quick' 'tail'\n",
      " 'wild']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# Initialize the CountVectorizer\n",
    "vectorizer_bow = CountVectorizer(max_features=200)\n",
    "bow_matrix = vectorizer_bow.fit_transform(cleaned_data)\n",
    "\n",
    "# Display the Bag of Words matrix\n",
    "print(\"Bag of Words:\\n\", bow_matrix.toarray())\n",
    "print(\"Feature Names:\", vectorizer_bow.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'quick': 7,\n",
       " 'brown': 1,\n",
       " 'fox': 4,\n",
       " 'jump': 5,\n",
       " 'lazy': 6,\n",
       " 'dog': 2,\n",
       " 'wild': 9,\n",
       " 'animal': 0,\n",
       " 'domestic': 3,\n",
       " 'tail': 8}"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer_bow.vocabulary_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Bag of Words:\n",
      " [[0 0 1 1 1 0 0 0 0 1 1 0 0 0 1 1 1 1 0 1 1 0 0 0]\n",
      " [0 0 0 0 1 0 0 1 0 1 0 1 0 0 0 0 1 0 1 1 0 0 0 0]\n",
      " [1 1 0 0 1 1 0 0 1 1 0 0 0 1 0 0 0 0 0 0 0 0 1 1]\n",
      " [0 0 0 0 1 0 1 0 0 1 0 0 1 0 0 0 0 0 0 0 0 1 0 0]]\n",
      "Feature Names: ['animal' 'animal dog' 'brown' 'brown fox' 'dog' 'dog domestic' 'dog fox'\n",
      " 'dog lazy' 'domestic' 'fox' 'fox jump' 'fox quick' 'fox tail' 'fox wild'\n",
      " 'jump' 'jump lazy' 'lazy' 'lazy dog' 'lazy fox' 'quick' 'quick brown'\n",
      " 'tail' 'wild' 'wild animal']\n"
     ]
    }
   ],
   "source": [
    "# Initialize the CountVectorizer\n",
    "vectorizer_bow = CountVectorizer(max_features=200,ngram_range=(1,2))    # addition of Ngram concept.\n",
    "bow_matrix = vectorizer_bow.fit_transform(cleaned_data)\n",
    "\n",
    "# Display the Bag of Words matrix\n",
    "print(\"Bag of Words:\\n\", bow_matrix.toarray())\n",
    "print(\"Feature Names:\", vectorizer_bow.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'quick': 19,\n",
       " 'brown': 2,\n",
       " 'fox': 9,\n",
       " 'jump': 14,\n",
       " 'lazy': 16,\n",
       " 'dog': 4,\n",
       " 'quick brown': 20,\n",
       " 'brown fox': 3,\n",
       " 'fox jump': 10,\n",
       " 'jump lazy': 15,\n",
       " 'lazy dog': 17,\n",
       " 'dog lazy': 7,\n",
       " 'lazy fox': 18,\n",
       " 'fox quick': 11,\n",
       " 'wild': 22,\n",
       " 'animal': 0,\n",
       " 'domestic': 8,\n",
       " 'fox wild': 13,\n",
       " 'wild animal': 23,\n",
       " 'animal dog': 1,\n",
       " 'dog domestic': 5,\n",
       " 'tail': 21,\n",
       " 'dog fox': 6,\n",
       " 'fox tail': 12}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer_bow.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pros:**\n",
    "\n",
    "- Captures word frequency.\n",
    "\n",
    "- Simple to implement.\n",
    "\n",
    "**Cons:**\n",
    "\n",
    "- Ignores word order and context.\n",
    "- Results in large, sparse matrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF:\n",
      " [[0.         0.51381313 0.268129   0.         0.268129   0.51381313\n",
      "  0.40509617 0.40509617 0.         0.        ]\n",
      " [0.         0.         0.3902801  0.         0.3902801  0.\n",
      "  0.58964518 0.58964518 0.         0.        ]\n",
      " [0.53114624 0.         0.27717414 0.53114624 0.27717414 0.\n",
      "  0.         0.         0.         0.53114624]\n",
      " [0.         0.         0.41988018 0.         0.41988018 0.\n",
      "  0.         0.         0.8046125  0.        ]]\n",
      "Feature Names: ['animal' 'brown' 'dog' 'domestic' 'fox' 'jump' 'lazy' 'quick' 'tail'\n",
      " 'wild']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "# Initialize the TfidfVectorizer\n",
    "vectorizer_tfidf = TfidfVectorizer()\n",
    "tfidf_matrix = vectorizer_tfidf.fit_transform(cleaned_data)\n",
    "\n",
    "# Display the TF-IDF matrix\n",
    "print(\"TF-IDF:\\n\", tfidf_matrix.toarray())\n",
    "print(\"Feature Names:\", vectorizer_tfidf.get_feature_names_out())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TF-IDF:\n",
      " [[0.         0.4472136  0.         0.         0.         0.4472136\n",
      "  0.         0.         0.         0.4472136  0.4472136  0.\n",
      "  0.4472136  0.        ]\n",
      " [0.         0.         0.         0.         0.57735027 0.\n",
      "  0.57735027 0.         0.         0.         0.         0.57735027\n",
      "  0.         0.        ]\n",
      " [0.5        0.         0.5        0.         0.         0.\n",
      "  0.         0.         0.5        0.         0.         0.\n",
      "  0.         0.5       ]\n",
      " [0.         0.         0.         0.70710678 0.         0.\n",
      "  0.         0.70710678 0.         0.         0.         0.\n",
      "  0.         0.        ]]\n",
      "Feature Names: ['animal dog' 'brown fox' 'dog domestic' 'dog fox' 'dog lazy' 'fox jump'\n",
      " 'fox quick' 'fox tail' 'fox wild' 'jump lazy' 'lazy dog' 'lazy fox'\n",
      " 'quick brown' 'wild animal']\n"
     ]
    }
   ],
   "source": [
    "# Initialize the TfidfVectorizer\n",
    "vectorizer_tfidf = TfidfVectorizer(ngram_range=(2,2))\n",
    "tfidf_matrix = vectorizer_tfidf.fit_transform(cleaned_data)\n",
    "\n",
    "# Display the TF-IDF matrix\n",
    "print(\"TF-IDF:\\n\", tfidf_matrix.toarray())\n",
    "print(\"Feature Names:\", vectorizer_tfidf.get_feature_names_out())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Pros:**\n",
    "\n",
    "- Balances word frequency with importance.\n",
    "\n",
    "- Reduces the weight of common words.\n",
    "\n",
    "**Cons:**\n",
    "\n",
    "- Still lacks semantic understanding.\n",
    "\n",
    "- Results in large, sparse matrices."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
